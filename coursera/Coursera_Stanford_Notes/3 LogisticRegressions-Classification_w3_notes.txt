***Logistic Regressions: Classification*** - w3 - Vid 1

Examples of classifications problems:
-Email: spam / not spam
-Online Transactions: Fraudulent (y/n)
-Tumor: Malignant/Benign

In most classification problems we are thinking of y as a true or false value (0,1). 
It could also stand for other values, but the numbers only refer to how to index that value
(e.g. 0 for benign, 1 for malignant, 2 for non-conclusive). Stated mathematically:

y ∈ {0,1}    % In general 0 is negative class (benign) and 1 is positive class (malignant).

We will start with 2 class (0 and 1) classification - binary classification problem, however, 
we will ultimately move into multi-class classification problems.

Threshold classifier outputs gives us the middle number to determine which values 
make y ∈ 0, and which make  y  ∈ 1. 
For instance, if we have a linear graph and say that the threshold value is 0.5 (y-axis).

We can try linear regression (fit a line to our data), 
we can determine which values on that line correspond to y ∈ 0 and y ∈ 1.
However, with this, because an outlier value can change the linear regression we apply, 
then we can get values of y that are either y >1 or y < 0 (seems weird). 
All we need to know from this is that there is another form of regression - 
logistic regression - which maps our data with a regression so that 

The old way: Linear Regression
Example:

h_θ(x) >= 0.5, predict y=1
h_θ(x) < 0.5, predict y=0

Remember that h_θ(x) also equals:

		  T
h_θ(x) = θ x

Logistic Regression is a classification algorithm.
With Logistic Regression, we will be able to classify h_θ(x) as:

0 <= h_θ(x) <= 1

***Logistic Regression Model*** - w3 - vid 2

Want 0 <= h_θ(x) <= 1

		    T
h_θ(x) = g(θ x)   %where g(z) = 1/(1+e^-z), when z is a real number.
This can be called the Sigmoid Function or the Logistic Function 
(this will turn into part of our Logistic Regression).

Thus g(θ' * x) = 1/(1+e^-(θ' * x))

See screenshot: w3 - SigmoidFunction_graph

*How to interpret the Hypothesis Output

h_θ(x) = estimated probability that y=1 on input of x (rather, h_θ(x)).

For example, if h_θ(x) = 0.7
We tell the patient that there is a %70 chance that the tumor is malignant.

h_θ(x) = P(y=1 | x;θ)   % our hypothesis equals the probability of y =1 of x parameterized by theta.
Rather, Dr. Ng's def - "the probability (P) that y = 1, given x, parameterized by θ."

Going to count on our hypothesis to give us estimates of the probability of y = 1. 

Since this is a classification class, we know that y can either be 0 or 1.

Because of this, we know:

P(y=0 | x;θ) + P(y=1 | x;θ)  = 1
Rearranging:

P(y=0 | x;θ)= 1 - P(y=1 | x;θ)

Now we know what Hypothesis representation of logistic regression, 
and we have seen what the formula is for the hypothesis function. 

***repeated notes from above - review but really good ***

Logistic Regression Model
Goal: want a hypothesis that is:
  0 <= h_θ(x) <= 1

The form of our hypothesis:
		    T
h_θ(x) = g(θ x)

The logistic function g(z) = 1/(1+e^-z)  where z is a real number, 
and in our original hypothesis:

g(h_θ(x)) = 1/(1+e^-(θ'x)    where theta prime is the transpose of theta, like Octave. 


graph g(z) or graph the sigmoid (logistic) function:
											|       __ -- -- -- -- -- -- -- g(z)
										  1 -      /
											|    /  
											|  / 
											|/
  The point of intersection is 0.5 -----   /|
     									  / |
     									/	|
	    					        _ /     |
     __ __ __ __ __ __ __ __ -- -- -		|
_________________________________________________________________________________________   
              							  0 |         
              								z     % this vertical axis is the z-axis
              								      % the values are between 0 and 1.

It asymptotes at 0 and 1. Thus we know that h_θ(x)is between 0 and 1 as well. 

Remember that h_θ(x) = estimated probability that y = 1 on input of x (rather, h_θ(x)).

So, if we know that there is a 70 percent chance, or a .7 probability of one of two outcomes, 
then we can calculate the probability for the other outcome, 
because we know that the probability must add up to be 1. 
For example, h_θ(x) = 0.7

Thus, 

h_θ(x) = P(y=1 | x;θ)   % probability that y=1, given x, parameterized by theta. 

y = 0 or y = 1          % thus we can compute the probability for zero. 

***Logistic Regression: Decision Boundary*** - vid 3

Want to better understand when this hypothesis will make predictions when y = 1and y = 0 
and better understand what the hypothesis looks like when we are dealing with multiple features. 

 h_θ(x) = g(θ'x) = P(y=1 | x;θ)
 
 Suppose predict y = 1 if h_θ(x) >= 0.5
 predict y = 0 if h_θ(x) < 0.5

Use the graph above to reference:

g(z) >= 0.5 when z >= 0

*This is key*  

So we can accurately say, that 
h_θ(x) = g(θ'x) >= 0.5  when θ'x >= 0 

h_θ(x) = g(θ'x) < 0.5   when θ'x < 0

To summarize, if we decide to predict if y is 1 or 0, depending upon whether the estimated 
probability P(y=1 | x;θ) is greater than, equal to, or less than 0.5 then that is the same as saying...
that we'll predict y = 1 when θ'x >= 0.5 and y = 0 when θ'x < 0.5

***Logistic Regression: Cost Function*** - vid 4
   We will talk about how to fit the parameters theta to the cost function. 
   
   
   training set {(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)}
   
   m examples and x is an element of the vector :
  								    [x_0]   where x_0 = 1, and y is an element of {0, 1}
								    |x_1|
								    |...|
								    [x_n] this is the Real numbers (R^n+1) to the n+1 term (because x_0 is included)
								
hypothesis: 	
	    
h_θ(x) = g(θ'*x) = 1/(1+e^-(θ'*x)	when  θ' * x = z	

How to choose parameters theta?
See the following screenshots
w3 - Log_Reg_Cost_Fnc_y=1	
w3 - Log_Reg_Cost_Fnc_y=0

				  {    -log(h_θ(x))    if  y = 1
*Cost(h_θ(x),y) = {										%Remember: y = 1 or y = 0 only.
				  {-log(1 - h_θ(x))    if  y = 0
					
***Logistic Regression: Simplified Cost Function & Gradient Descent***    - vid 5

Most commonly used form of Regression.

Please see screenshot:

w3 - Log_Reg_Grad_Desc_Algo

You will need to be able to take the vectorized form and code with it:

θ := θ - a * (1/m) * ∑ i=1 to m [(h_θ(x^(i))−y^(i)) * x^(i)]

The main difference between logistic and linear regression is that our hypothesis will be different:

Lin Reg:
		  T
h_θ(x) = θ x

Log Reg:

h_θ(x) = 1/(1+e^-(θ' * x))

***Logistic Regression: Advanced optimization*** - vid 6

With our logistic regression algorithm, we want the min of our cost function J(θ).

We need to supply code for J(θ) and derivative of J(θ). 
We really only need code to compute the derivative term just like in:
θ := θ - a * (1/m) * ∑ i=1 to m [(h_θ(x^(i))−y^(i)) * x^(i)]

Types of optimization algorithms:
-Gradient descent
-Conjugate gradient
-BFGS
-L-BFGS

Advantages of the bottom three types (-Conjugate gradient -BFGS -L-BFGS):
- no need to manually pick a
-often faster than gradient descent
Disadvantages:
-More complex

*See screenshot w3 - How_Use_Log_Reg

*The example for how to code in Octave

DO FIRST: put the following in a .m function file. 
NOTE: Double check that you are in the correct directory in order to run the function file. 

function [jVal,gradient] = costFunction(theta);

jVal = ((theta(1)-5)^2+(theta(2)-5)^2);
gradient = zeros(2,1);
gradient(1) = 2 * (theta(1)-5);
gradient(2) = 2 * (theta(2)-5);

DO SECOND: run the following in Octave:

> options = optimSet('GradObj', 'on', 'MaxIter', '100');
> initialTheta = zeros(2,1);
> ontTheta, functionVal, exitFlag = fminunc(@costFcuntion, initialTheta, options)

This is how to optimize our example, now we will apply this to logistic regression.

*Please see screenshot: w3 - Opt_Ex_Code_Input_Struc
*and: w3 - Ex_Log_Reg_Quiz_Prob-Assign#2

***Logistic Regress: Multi Class Classification*** - vid 7
Examples:
- put email into different folders or different tags.
-weather: cloudy, rainy, foggy, snowy, warm, hot, etc.

One versus all classification (one-vs-all) aka (one-vs-rest):

Turn our multi sets into different classes. 

*** Regularization: The Problem of Overfitting *** - Vid 8

Overfitting: If we have too many features, the learned hypothesis may fit the training
set very well (J(θ) = (1/2m) * ∑ i=1 to m (h_θ(x^(i))−y^(i))^2 ~= [approx] 0), but fails 
to generalize to new examples (predict prices n new examples). 

*** Regularization: The Cost Function *** - Vid 9

Regularization Term: (λ * ∑ j=1 to n [θ^2])]

Regularized cost function:

J(θ) = (1/2m) * ∑ i=1 to m [(h_θ(x^(i))−y^(i))^2 + (λ * ∑ j=1 to n [θ^2])]

When lambda is very large (e.g. 10^10) then you can cause many or all of the theta values to 
become approximately 0, which will in turn cause an underestimated model. 

*** Regularization: Regularized Linear Regression *** - Vid 10

Gradient Descent with Regularization:

Repeat {
θ_j := θ_j * (1 - a(λ/m)) - a(1/m) * ∑ i=1 to m [(h_θ(x^(i))−y^(i)) * x^(i)]
}

*** Regularization: Regularized Logistic Regression *** - Vid 11








 






								























































